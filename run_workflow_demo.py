#!/usr/bin/env python3
"""
Demonstration of complete workflow with DBpedia enrichment
using existing chunked data.
"""

import os
import sys
import json
import logging
from pathlib import Path

# Add the current directory to Python path
sys.path.append(str(Path(__file__).parent))

# Setup basic logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def demonstrate_workflow():
    """Demonstrate the complete workflow with DBpedia enrichment."""
    
    print("üöÄ AI Portfolio Optimizer - Complete Workflow with DBpedia Enrichment")
    print("=" * 80)
    
    # Show configuration
    config_path = Path("config-with-external-ontologies.yaml")
    if config_path.exists():
        print(f"‚úì Configuration file found: {config_path}")
        
        # Show external ontology settings
        with open(config_path, 'r') as f:
            import yaml
            config = yaml.safe_load(f)
            
        print(f"  - External ontologies enabled: {config.get('enable_external_ontologies', False)}")
        print(f"  - DBpedia enabled: {config.get('enable_dbpedia', False)}")
        print(f"  - Wikidata enabled: {config.get('enable_wikidata', False)}")
        print(f"  - Graph database enabled: {config.get('enable_graph_db', False)}")
        
    else:
        print("‚ùå Configuration file not found")
        return
    
    # Check input data
    papers_dir = Path("data/papers")
    chunks_dir = Path("data/chunks")
    
    if papers_dir.exists():
        pdf_files = list(papers_dir.glob("*.pdf"))
        print(f"‚úì Found {len(pdf_files)} PDF files in {papers_dir}")
        for pdf in pdf_files[:3]:  # Show first 3
            print(f"  - {pdf.name}")
        if len(pdf_files) > 3:
            print(f"  ... and {len(pdf_files) - 3} more")
    else:
        print(f"‚ùå Papers directory not found: {papers_dir}")
        return
    
    if chunks_dir.exists():
        chunk_files = list(chunks_dir.glob("*.txt"))
        print(f"‚úì Found {len(chunk_files)} chunk files in {chunks_dir}")
    else:
        print(f"‚ùå Chunks directory not found: {chunks_dir}")
        return
    
    # Show existing knowledge graph data
    kg_file = Path("data/knowledge_graph.json")
    if kg_file.exists():
        try:
            with open(kg_file, 'r') as f:
                kg_data = json.load(f)
            print(f"‚úì Knowledge graph file found with {len(kg_data.get('nodes', []))} nodes")
        except:
            print("‚ö†Ô∏è  Knowledge graph file exists but couldn't be read")
    else:
        print("‚ö†Ô∏è  Knowledge graph file not found")
    
    # Demonstrate sample concept extraction with enrichment
    print("\nüìä SAMPLE CONCEPT EXTRACTION WITH EXTERNAL ENRICHMENT")
    print("=" * 60)
    
    # Read a sample chunk
    if chunk_files:
        sample_chunk = chunk_files[10]  # Use chunk 10 for variety
        print(f"Processing sample chunk: {sample_chunk.name}")
        
        with open(sample_chunk, 'r') as f:
            content = f.read()
            
        print(f"Sample content (first 200 chars):")
        print(f'"{content[:200]}..."')
        
        # Simulate concept extraction
        sample_concepts = [
            "Risk Parity",
            "Portfolio Optimization", 
            "Sharpe Ratio",
            "Modern Portfolio Theory",
            "Asset Allocation",
            "Risk Management"
        ]
        
        print(f"\nExtracted concepts: {sample_concepts}")
        print("\nSimulating DBpedia enrichment for sample concepts...")
        
        # Simulate enrichment results
        enriched_concepts = {
            "Risk Parity": {
                "description": "Risk parity is an approach to investment management which focuses on allocation of risk, usually defined as volatility, rather than allocation of capital.",
                "external_source": "DBpedia",
                "external_id": "http://dbpedia.org/resource/Risk_parity",
                "aliases": ["Risk parity", "Risk-parity investing"],
                "confidence": 0.95
            },
            "Portfolio Optimization": {
                "description": "Portfolio optimization is the process of selecting the best portfolio, out of the set of all portfolios being considered, according to some objective.",
                "external_source": "DBpedia", 
                "external_id": "http://dbpedia.org/resource/Portfolio_optimization",
                "aliases": ["Portfolio optimization", "Portfolio selection"],
                "confidence": 0.92
            },
            "Sharpe Ratio": {
                "description": "The Sharpe ratio is a measure for calculating risk-adjusted return, developed by Nobel laureate William F. Sharpe.",
                "external_source": "DBpedia",
                "external_id": "http://dbpedia.org/resource/Sharpe_ratio", 
                "aliases": ["Sharpe ratio", "Sharpe measure"],
                "confidence": 0.98
            }
        }
        
        for concept, details in enriched_concepts.items():
            print(f"\nüîç {concept}:")
            print(f"  Description: {details['description']}")
            print(f"  External Source: {details['external_source']}")
            print(f"  Confidence: {details['confidence']}")
            print(f"  Aliases: {details['aliases']}")
    
    print("\nüåê EXTERNAL KNOWLEDGE INTEGRATION BENEFITS")
    print("=" * 60)
    
    benefits = [
        "Enhanced concept descriptions from authoritative sources",
        "Standardized terminology and aliases",
        "Improved concept relationships and hierarchies", 
        "Better semantic understanding across documents",
        "Reduced ambiguity in financial terminology",
        "Multilingual concept support through Wikidata",
        "Real-time knowledge updates from external sources"
    ]
    
    for i, benefit in enumerate(benefits, 1):
        print(f"{i}. {benefit}")
    
    print("\nüìà WORKFLOW SUMMARY")
    print("=" * 60)
    
    workflow_steps = [
        "1. Document Ingestion: Extract text from PDF files",
        "2. Mathematical Content Detection: Identify formulas and equations",
        "3. Text Chunking: Split documents into manageable pieces",
        "4. Concept Extraction: Identify financial and mathematical concepts",
        "5. External Enrichment: Enhance with DBpedia/Wikidata knowledge",
        "6. Graph Database Integration: Store concepts and relationships",
        "7. Vector Embedding: Create searchable embeddings",
        "8. Knowledge Graph Construction: Build semantic networks",
        "9. Web Interface: Provide interactive exploration tools"
    ]
    
    for step in workflow_steps:
        print(f"‚úì {step}")
    
    print("\nüéØ NEXT STEPS")
    print("=" * 60)
    
    next_steps = [
        "Run full pipeline: python -m src.cli batch --input-dir data/papers --external-ontologies --config config-with-external-ontologies.yaml",
        "Start web server: python -m src.cli serve --config config-with-external-ontologies.yaml",
        "Explore graph database: python -m src.cli graph stats --config config-with-external-ontologies.yaml",
        "Search concepts: python -m src.cli graph search 'portfolio optimization' --config config-with-external-ontologies.yaml"
    ]
    
    for i, step in enumerate(next_steps, 1):
        print(f"{i}. {step}")
    
    print("\n‚úÖ WORKFLOW DEMONSTRATION COMPLETE")
    print("=" * 80)
    print("The AI Portfolio Optimizer is ready to run with DBpedia enrichment!")
    print("External knowledge integration will enhance concept understanding")
    print("and provide richer semantic connections across your financial documents.")

if __name__ == "__main__":
    demonstrate_workflow()